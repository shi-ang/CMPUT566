# THOUGHT QUESTIONS 2
1. $l_1$ regularizer reduces model complexity by making parameters sparsely (reducing the number of parameters), and $l_2$ regularizer reduces model complexity by reducing the value of the parameters. Can you combine these two regularizers to a new penalty term to obtain a simple model more efficiently? Empirically speaking, if we want a model to be both sparse and have small weight values (prevent from overfitting), should we combine the two regularizers to update the weights or conducting the $l_1$ and $l_2$ updates to the weights separately? 


2. The stochastic gradient descent algorithm randomly selects one sample from the training set to learn each time. If we do not change the learning rate as the weight updating, how can we have the confidence that the loss/cost will converge to the global minima? Due to the randomness of the selected samples, these following scenarios will happen:  (i) The loss/cost will converge to a local minima instead of the global minima because the first few samples have some noise.  (ii) Even if we can update the loss/cost close to the global minima, it always jumps around the global. Because for a random sample, their gradient values do not equal to 0 in the most advantageous point (local minima, global minima and saddlepoint).