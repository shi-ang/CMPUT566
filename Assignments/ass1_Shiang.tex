\documentclass[11pt]{article}

\newcommand{\semester}{Fall 2019}
\usepackage{fancyhdr,multicol}
\usepackage{amsmath,amssymb}

%%set listing package
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%% Custom page layout.
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-2in}
\setlength{\topmargin}{-.5in}
\setlength{\headsep}{.5in}
\addtolength{\headsep}{-\headheight}
\setlength{\footskip}{.5in}
\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\flushbottom

\allowdisplaybreaks

%% Custom headers and footers.
\pagestyle{fancyplain}
\let\headrule=\empty
\let\footrule=\empty
\lhead{\fancyplain{}{\semester}}
\rhead{\fancyplain{}{CMPUT 466/566: Machine Learning}}
\cfoot{{\thepage/\pageref{EndOfAssignment}}}

%% Macros to generate question and part numbers automatically
\newcounter{totalmarks}
\setcounter{totalmarks}{0}
\newcounter{questionnumber}
\setcounter{questionnumber}{0}
\newcounter{subquestionnumber}[questionnumber]
\setcounter{subquestionnumber}{0}
\renewcommand{\thesubquestionnumber}{(\alph{subquestionnumber})}
\newcommand{\question}[2][]%
  {\ifx\empty#2\empty\else
   \addtocounter{totalmarks}{#2}\refstepcounter{questionnumber}\fi
   \bigskip\noindent\textbf{\Large Question \thequestionnumber. } #1
   {\scshape\ifx\empty#2\empty(continued)\else
   [#2 mark\ifnum #2 > 1 s\fi]\fi}\par
   \medskip\noindent\ignorespaces}
\newcommand{\subquestion}[2][]%
  {\ifx\empty#2\empty\else\refstepcounter{subquestionnumber}\fi
   \medskip\noindent\textbf{\large \thesubquestionnumber } #1
   {\scshape\ifx\empty#2\empty(continued)\else
   [#2 mark\ifnum #2 > 1 s\fi]\fi}
   \smallskip\noindent\ignorespaces}
\newcommand{\bonus}[2][]%
  {\bigskip\noindent\textbf{\Large Bonus. } #1
   {\scshape\ifx\empty#2\empty(continued)\else
   [#2 mark\ifnum #2 > 1 s\fi]\fi}\par
   \medskip\noindent\ignorespaces}

% Enable final count to be printed at top
\usepackage{totcount}
\regtotcounter{totalmarks}
\begin{document}

\thispagestyle{plain}

\begin{center}
\bfseries
{\Large Homework Assignment \# 1}\\
   Due: Thursday, September 26, 2019, 11:59 p.m. \\
   Total marks: \total{totalmarks}
\end{center}



\question{10}

Let $X$ be a random variable with outcome space $\Omega=\{a,b,c\}$ and  $p(a)=0.1, p(b)=0.2$, and $p(c)=0.7$. Let 
%
\begin{align*}
f(x)=
\left\{
  \begin{array}{lr}
    10 &  \text{if } x = a\\
    5 &  \text{if } x = b\\
    10/7 & \text{if } x = c
  \end{array}
\right.
\end{align*}

\subquestion{3} 
What is $E[f(X)]$?

\textbf{Answer:} 
\begin{align*}
    E[f(X)] = 0.1 \times 10 + 0.2 \times 5 + 0.7 \times \frac{10}{7} = 3
\end{align*}

\subquestion{3} 
What is $E[1/p(X)]$?

\textbf{Answer:} 
\begin{align*}
    E[1/p(X)] = \sum_{x\in X}^{}\frac{1}{p(X)} p(X) = \sum_{x\in X} 1 = 3
\end{align*}

\subquestion{4} 
For an arbitrary pmf $p$, what is $E[1/p(X)]$?

\textbf{Answer:} \\
For any arbitrary pmf: 
\begin{align*}
    E[1/p] = \sum_{x \in X}\frac{1}{p(X)}p(X) = \sum_{x \in X} 1 = n
\end{align*}
$n$ is the number of discrete outcome spaces.
\newpage












% You can also define new variables to make it easier
% and avoid long commands
\newcommand{\muvec}{\boldsymbol{\mu}}

\question{15}
Let $\mathbf{X}_1, \ldots, \mathbf{X}_m$ be independent multivariate Gaussian random variables, with $\mathbf{X}_i \sim \mathcal{N}(\muvec_i, \boldsymbol{\Sigma}_i)$, with $\muvec_i \in \mathbb{R}^d$ and $\boldsymbol{\Sigma}_i \in \mathbb{R}^{d \times d}$ for dimension $d \in \mathbb{N}$. 
Define $\mathbf{X} = a_1 \mathbf{X}_1 + a_2 \mathbf{X}_2 + \ldots + a_m \mathbf{X}_m$ as a convex combination, $a_i \ge 0$ and $\sum_{i=1}^m a_i = 1$. 

\subquestion{5}
Write the expected value $E[\mathbf{X}]$ in terms of the givens $a_i, \muvec_i, \boldsymbol{\Sigma}_i$. Show all you steps.
What is the dimension of $E[\mathbf{X}]$?

\textbf{Answer:} 
\begin{align*}
    E[\mathbf{X}] 
    &= E[a_1 \mathbf{X}_1 + a_2 \mathbf{X}_2 + \ldots + a_m \mathbf{X}_m] \\
    &= a_1 E[\mathbf{X}_1] + a_2 E[\mathbf{X}_2] + \ldots + a_m E[\mathbf{X}_m] \\
    &= a_1 \muvec _1 + a_2 \muvec _2 + \ldots + a_m \muvec _m
\end{align*}
where $E[\mathbf{X}] \in \mathbb{R}^{d}$.

\subquestion{10}
Write the covariance $\text{Cov}[\mathbf{X}]$ in terms of the givens $a_i, \muvec_i, \boldsymbol{\Sigma}_i$. Show all you steps.
What is the dimension of $\text{Cov}[\mathbf{X}]$?
Briefly explain how the result for
$\text{Cov}[\mathbf{X}]$ would be different if the variables
$\mathbf{X}_1$ and $\mathbf{X}_2$ are not independent and have covariance
$\text{Cov}[\mathbf{X}_1,\mathbf{X}_2] = \boldsymbol{\Lambda}$ for $\boldsymbol{\Lambda} \in \mathbb{R}^{d \times d}$.

\textbf{Answer:} 
\begin{align*}
    \text{Cov}[\mathbf{X}] 
    &= \text{Cov}[a_1 \mathbf{X}_1 + a_2 \mathbf{X}_2 + \ldots + a_m \mathbf{X}_m]\\
    &= \sum_{i = 1}^{m} V[\mathbf{X}_i] + 2 \sum_{1 \leq i < j \leq m} \text{Cov}[\mathbf{X}_i, \mathbf{X}_j]
\end{align*}

If $\mathbf{X}_1, \ldots, \mathbf{X}_m$ are independent random variables, then $\text{Cov}[\mathbf{X}_i, \mathbf{X}_j] = 0$ $\forall i,j$.
\begin{align*}
    \text{Cov}[\mathbf{X}] = a_1^2\boldsymbol{\Sigma}_1 + a_2^2\boldsymbol{\Sigma}_2 + \ldots + a_m^2\boldsymbol{\Sigma}_m
\end{align*}

For the second question, if variables $\mathbf{X}_1$ and $\mathbf{X}_2$ are not independent, 
\begin{align*}
    \text{Cov}[\mathbf{X}] = a_1^2\boldsymbol{\Sigma}_1 + a_2^2\boldsymbol{\Sigma}_2 + \ldots + a_m^2\boldsymbol{\Sigma}_m + 2a_1 a_2 \boldsymbol{\Lambda}
\end{align*}
where the dimension of $\text{Cov}[\mathbf{X}] \in \mathbb{R}^{d \times d}$
\newpage



















\question{15}

This question involves some simple simulations, to better visualize
random variables and get some intuition for sampling,
which is a central theme in machine learning. 
Use the attached code called \verb+simulate.py+.
This code is a simple script for sampling and plotting
with python; play with some of the parameters to see what it is doing.
Calling \verb+simulate.py+ runs with default parameters;
\verb+simulate.py 1 100+ simulates 100 samples from a 1d Gaussian. 
The generated plot is generically for 3 dimensions. If you call the function with 1d,
then it simply plots the points on a line, but on a 3-dimensional plot. The maximum
dimension that can be given to the script is 3.

Note that if you do not have matplotlib installed, you will have to install it. 

\subquestion{5}
Run the code for 10, 100 and 1000 samples with dim=1 and $\sigma = 1.0$.
Next run the code for 10, 100 and 1000 samples with dim=1 and $\sigma = 10.0$.
What do you notice about the sample mean?

\textbf{Answer:} \\
The mean value converges to $0$ as the sample number goes up.

\subquestion{5}
The current covariance for dim=3 is 
%
\begin{align*}
\Sigma = \left[ \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1\end{array} \right]
.
\end{align*}
%
%
What does that mean about the multivariate Gaussian (i.e., the vector random variable composed of random variables $X$, $Y$ and $Z$)?

\textbf{Answer:} \\
$\text{Corr}[X,Y] = \text{Corr}[Y,Z] =\text{Corr}[X,Z] = 0$ means that the three random variables have no correlation with each other. However, it does not mean they are independent with each other. 

\subquestion{5}
Change the covariance to
%
\begin{align*}
\Sigma = \left[ \begin{array}{ccc}
1 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 1\end{array} \right]
.
\end{align*}
%
%
What happens?

\textbf{Answer:} \\
The three dimensional data group compresses to a two dimensional dataset.\\
Because $\eta_{XZ}=\text{Corr}[X,Z]=\frac{\text{Cov}[X,Z]}{\sqrt{V[X]}\sqrt{V[Z]}} = 1$, which means that random variables $X$ and $Z$ are fully correlated.


\newpage





















\question{30}

Suppose that the number of accidents occurring daily in a certain plant has a 
Poisson distribution with an unknown mean $\lambda$. 
Based on previous experience in similar industrial plants, 
suppose that our initial feelings about the possible value of $\lambda$ 
can be expressed by an exponential distribution with parameter $\theta=\tfrac{1}{2}$. 
That is, the prior density is
%
\begin{align*}
p(\lambda)=\theta \textrm{e}^{-\theta\lambda}
\end{align*}
%
%
where $\lambda\in [0,\infty)$. 

\subquestion{5} 
Before observing any data (any reported accidents),
what is the most likely value for $\lambda$?

\textbf{Answer:}\\
The most likely value for $\lambda$ has the highest probability. Therefore, 
\begin{align*}
\text{argmax}p(\lambda) = \text{argmax}\theta \textrm{e}^{-\theta\lambda}
\\\Rightarrow \lambda = 0, \space p(\lambda = 0) = 0.5
\end{align*}

\subquestion{5} 
Now imagine there are 79 accidents over 9 days. 
Determine the maximum likelihood estimate of $\lambda$.

\textbf{Answer:}\\
We will estimate the $\lambda$ as:
\begin{align*}
    \lambda_{\mathbf{MLE}} = \text{argmax}_{\lambda \in (0, \infty)}p(\mathcal{D}|\lambda)
\end{align*}

\begin{align*}
    p(\mathcal{D}|\lambda) \\
    &= p({\{x_i\}}_{i=1}^{n}|\lambda) \\
    &= \prod_{i-1}^{n}p(x_i|\lambda) \\
    &= \frac{\lambda^{\sum_{i=1}^{n} x_i} \cdot \textrm{e}^{-n \lambda}}{\prod_{i=1}^{n} x_i!}
\end{align*}

The log-likelihood $ll(\mathcal{D}, \lambda)$ can be expressed as 
\begin{align*}
    ll(\mathcal{D}, \lambda) = \text{ln} \lambda \sum_{i=1}^{n}x_i - n\lambda - \sum_{i=1}^{n}\text{ln}(x_i !)
\end{align*}

The first derivative is:
\begin{align*}
    \frac{\partial ll (\mathcal{D}, \lambda)}{\partial \lambda} = \frac{1}{\lambda}\sum_{i=1}^{n} x_i - n = 0\\
    \Rightarrow \lambda_{\mathbf{MLE}} = \frac{1}{n} \sum_{i=1}^{n} x_i =\frac{79}{9} \approx 8.8
\end{align*}


\subquestion{5} 
Again imagine there are 79 accidents over 9 days. 
Determine the maximum a posteriori (MAP) estimate of  $\lambda$.

\textbf{Answer:}\\
The likelihood function would be the same as the last question, the prior distribution isï¼š
\begin{align*}
    p(\lambda)
    &= \theta \textrm{e}^{-\theta\lambda} \text{ln}p(\lambda|\mathcal{D})\propto \text{ln}p(\mathcal{D|\lambda}) + \text{ln}p(\lambda)) \\
    &= \text{ln}\lambda \sum_{i=1}^{n}x_i- n\lambda - \sum_{i=1}^{n}\text{ln}(x_i!) + \text{ln}\theta - \theta \lambda
\end{align*}

\begin{align*}
    \text{ln}p(\lambda|\mathcal{D})\propto \text{ln}p(\mathcal{D|\lambda}) + \text{ln}p(\lambda)) = \text{ln}\lambda \sum_{i=1}^{n}x_i- n\lambda - \sum_{i=1}^{n}\text{ln}(x_i!) + \text{ln}\theta - \theta \lambda
\end{align*}

The first derivative is:
\begin{align*}
    \frac{\partial ll (\mathcal{D}, \lambda)}{\partial \lambda} &= \frac{1}{\lambda}\sum_{i=1}^{n}x_i-n-\theta = 0 \\
    \Rightarrow \lambda_{\mathbf{MLE}} &= \frac{\sum_{i=1}^{n}x_i}{n+\theta} = \frac{79}{9.5} \approx 8.3
\end{align*}


\subquestion{5}
Imagine you now want to predict the number of accidents for tomorrow. 
How can you use the maximum likelihood estimate computed above?
What about the MAP estimate? What would they predict?

\textbf{Answer:} \\
For $\lambda_{\mathbf{MLE}} = 79/9$, we have the Poisson distribution:
\begin{align*}
    p(x) &= \frac{\frac{79}{9}^{x} \cdot e^{-\frac{79}{9}}}{x!}\\
    \text{argmax}_{x \in \{ 0, 1, 2, \ldots \}}p(x) &\Rightarrow p(x=8) = 0.135
\end{align*}
We could use this to predict the most likelihood number of accidents for tomorrow is 8. For $\lambda_{\mathbf{MLE}} = 79/9.5$, we have the Poisson distribution:
\begin{align*}
    p(x) &= \frac{\frac{79}{9.5}^{x} \cdot e^{-\frac{79}{9.5}}}{x!}\\
    \text{argmax}_{x \in \{ 0, 1, 2, \ldots \}}p(x) &\Rightarrow p(x=8) = 0.139
\end{align*}
We could also use this to predict the most likelihood number of accidents for tomorrow, which is also 8 but with different probability value.\\
The solution of MLE is based on known samples. It hopes that by adjusting the model parameters, the model can maximize the probability of occurrence of the sample condition. In this example, using the MLE solution of $\lambda$ predicts the number of accidents with most highest probability based on only the data sample.\\
MAP is also based on known samples, but also adding a prior knowledge. That is, the model parameters may satisfy a certain distribution and no longer depend entirely on the data sample. In this example, using the MAP solution of $\lambda$ predicts the number of accidents with the most highest probability based on both the data sample and prior knowledge of $\lambda$.

\subquestion{5}
For the MAP estimate, what is the purpose of the prior once we observe this data?

\textbf{Answer:}\\
For small data samples or data sets with noise and error, the model cannot be fully optimized by the data. An addition prior can greatly improved this problem. On the other hand, when the sample size is extremely large, the results of MLE and MAP tend to be the same.

\subquestion{5} 
Imagine that now new safety measures have been put in place
and you believe that the number of accidents per day should sharply
decrease. How might you change $\theta$ to better reflect this new belief about the number of accidents?
\emph{Hint:} Look at the plots of some exponential distributions to better understand
the prior chosen on $\lambda$. 

\textbf{Answer:}\\
For the exponential distribution $p(\lambda)$, the expect value equals to $E[p(\lambda)]=\frac{1}{\theta}$. If we want the number of accidents per day sharply decreasing, then the expect value should be decrease as well, which means $\theta$ should get higher.


\newpage
























\question{30}

Imagine that you would like to predict 
if your favorite table will be free at your favorite restaurant. 
The only additional piece of information you can collect, however, is if
it is sunny or not sunny.
%
You collect paired samples from visit of the form (is sunny, is table free),
where it is either sunny (1) or not sunny (0) 
and the table is either free (1) or not free(0).

\subquestion{10}
How can this be formulated as a maximum likelihood problem?

\textbf{Answer:}\\
Think as we are given two binary random variables $X$ (sunny or not) and $Y$ (free table or not).
The data is distributed as a Bernoulli distribution with:
\begin{align*}
    p(y|X) = 
    \left\{
        \begin{array}{lr}
        \alpha &   y =  \text{table is free}\\
        1-\alpha &   y = \text{table is not free}
        \end{array}
    \right.
\end{align*}
Then our goal is to estimate this unknown parameter $\sigma$ for each outcome value of $X$.

\subquestion{10}
Assume you have collected data for the last 10 days and computed the maximum likelihood solution to
the problem formulated in (a). 
If it is sunny today, how would you predict if your table will be free?

\textbf{Answer:}\\
If we have the data for last 10 days and get the MLE solution, we can form a distribution mass function as following:
\begin{align*}
    p(y|x= Sunny) = 
    \left\{
        \begin{array}{lr}
        \alpha &  y =  \text{table is free}\\
        1-\alpha &  y = \text{table is not free}
        \end{array}
    \right.
\end{align*}

\begin{align*}
    p(y|x=Not Sunny) = 
    \left\{
        \begin{array}{lr}
        \beta &   y =  \text{table is free}\\
        1-\beta &   y = \text{table is not free}
        \end{array}
    \right.
\end{align*}
where $\alpha$ and $\beta$ are two constants.
If today is sunny and $\alpha$ is larger than $(1-\alpha)$, we can make a prediction about the table is free. Otherwise, the table is not free.

\subquestion{10}
Imagine now that you could further gather information about if it is morning, afternoon, or evening.
How does this change the maximum likelihood problem?

\textbf{Answer:}\\
The new information can be treated as a third random variable $Z$ with outcome spaces \{morning, afternoon, evening\}. Therefore, the new conditional distribution for the event can be expressed as:
\begin{align*}
    p(y|X, Z) = 
    \left\{
        \begin{array}{lr}
        \alpha &   y =  \text{table is free}\\
        1-\alpha &   y = \text{table is not free}
        \end{array}
    \right.
\end{align*}
Then our goal is to estimate the MLE of this unknown parameter $\sigma$ for each outcome value of $X$ and $Z$.


\newpage

























\bonus{20}

\subquestion{10}
Using a computer, generate 1000 samples from a $d$-dimensional multivariate Gaussian with mean $\boldsymbol{0}$ and identity covariance matrix. Compute the average $\ell_2$ distance of each sample to the origin. Repeat this experiment for $d \in \{1, 2, 4, 8, 16, 32, 64, 128, 256\}$. 
What happens to the average distance as $d$ increases? For $k$-means clustering, what is one implication of this outcome?

\textbf{Answer:}\\
The code for generate multivariate Gaussian samples is listed as follow:
\begin{lstlisting}
    import numpy as np

    dim = 256   #1, 2, 4, 8, 16, 32, 64, 128, 256
    numSamples = 1000
    sigma = np.identity(dim)
    mean = np.zeros(dim)
    np.random.seed(515)
    samples =  np.random.multivariate_normal(mean, sigma, numSamples).T
    l2 = np.mean(np.sum(np.power(samples, 2),axis =0)**(1./2))
    print(l2)
\end{lstlisting}
The average $\ell_2$ distance are \{0.752, 1.22, 1.85, 2.71, 3.86, 5.56, 7.93, 11.24, 15.94\} with the increment of dimension. So the answer is that the average $\ell_2$ increases as $d$ increases.\\
As to the implication of $k$-means clustering, the larger the dimension is, the further the data will be from the center/mean (which also means the more sparse the group of points/data will be). It also means that high dimensional data makes the model more difficult to optimize.

\subquestion{10}
Consider two $d$-dimensional hypercubes centered at the origin. The first has a side length of 1 and the second has a side length of 1 - $\epsilon$ ($ 0 < \epsilon < 1$). Give an expression for the ratio of the volume of the second hypercube to the volume of the first (in terms of $d$ and $\epsilon$). 
What happens as $d$ gets large? How does this help explain the result about average distances from the previous question?

\textbf{Answer:}\\
The ratio of the volume can be expressed as:
\begin{align*}
    (1-\epsilon)^{d}
\end{align*}
If $d$ gets larger, the ratio of the volume gets smaller.
To simplify the question, we take $1d$, $2d$, and $3d$ hypercudes as some examples. The density of the data sets will be come more sparse if we take 1000 samples in a $1$ length line, a $1\times1$ area square, and a $1\times1\times1$ volume cube, respectively. The $\ell_2$ distance will get larger.

\label{EndOfAssignment}%

\end{document}
